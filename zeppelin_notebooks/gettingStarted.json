{"paragraphs":[{"text":"%dep\n//this must be in the first cell of a notebook\nz.reset()\n\n// Add spark-csv package\nz.load(\"com.databricks:spark-csv_2.10:1.3.0\")","dateUpdated":"Jan 10, 2016 1:49:22 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452431644551_-1140476489","id":"20160110-081404_1702252778","dateCreated":"Jan 10, 2016 8:14:04 AM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:19"},{"text":"%md\n#Getting started with Spark\n\n### In scope\n* Install locally\n* usefull tools\n* reading/writing data\n\n### Out of scope\n* cluster settings\n* streaming\n* Dick Cheney's unknown's unknown","dateUpdated":"Jan 10, 2016 4:41:05 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452431870837_1292461017","id":"20160110-081750_1865869751","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Getting started with Spark</h1>\n<h3>In scope</h3>\n<ul>\n<li>Install locally</li>\n<li>usefull tools</li>\n<li>reading/writing data</li>\n</ul>\n<h3>Out of scope</h3>\n<ul>\n<li>cluster settings</li>\n<li>streaming</li>\n<li>Dick Cheney's unknown's unknown</li>\n</ul>\n"},"dateCreated":"Jan 10, 2016 8:17:50 AM","dateStarted":"Jan 10, 2016 4:41:05 PM","dateFinished":"Jan 10, 2016 4:41:05 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:694","focus":true},{"text":"%md\n## Local Installation\n* Dowload pre-build http://spark.apache.org/downloads.html\n* Choose newest here `(1.5.2) pre-build for hadoop 2.6 and later` but I tried 1.6 over the weekend\n* Untar wherever you like (I did `$HOME/local_spark/`)\n* [path-to-spark]  `./bin/spark-shell`\n    * `--master` to set the master\n        * `local` locally,  one worker thread\n        * `local[n]` locally, `n` worker thread (`n= * ` mean # of core available)\n        *  `[HOST]PORT:` \n        *  ...\n* http://spark.apache.org/docs/latest/\n    1. e.g. \n","dateUpdated":"Jan 10, 2016 4:07:37 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452457974542_43007618","id":"20160110-153254_1377764816","dateCreated":"Jan 10, 2016 3:32:54 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:677","focus":true,"dateFinished":"Jan 10, 2016 4:07:37 PM","dateStarted":"Jan 10, 2016 4:07:37 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Local Installation</h2>\n<ul>\n<li>Dowload pre-build http://spark.apache.org/downloads.html</li>\n<li>Choose newest here <code>(1.5.2) pre-build for hadoop 2.6 and later</code> but I tried 1.6 over the weekend</li>\n<li>Untar wherever you like (I did <code>$HOME/local_spark/</code>)</li>\n<li>[path-to-spark]  <code>./bin/spark-shell</code><ul>\n<li><code>--master</code> to set the master<ul>\n<li><code>local</code> locally,  one worker thread</li>\n<li><code>local[n]</code> locally, <code>n</code> worker thread (<code>n= *</code> mean # of core available)</li>\n<li><code>[HOST]PORT:</code></li>\n<li>&hellip;</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>http://spark.apache.org/docs/latest/<ol>\n<li>e.g.</li>\n</ol>\n</li>\n</ul>\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452458565906_-803762891","id":"20160110-154245_135857012","dateCreated":"Jan 10, 2016 3:42:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:713","text":"%md\n## Shell\n* **scala** `./bin/spark-shell` (a modified version of the Scala shell)\n* **pyspark** `./bin/pyspark`","dateUpdated":"Jan 10, 2016 3:47:16 PM","dateFinished":"Jan 10, 2016 3:47:16 PM","dateStarted":"Jan 10, 2016 3:47:16 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Shell</h2>\n<ul>\n<li><strong>scala</strong> <code>./bin/spark-shell</code> (a modified version of the Scala shell)</li>\n<li><strong>pyspark</strong> <code>./bin/pyspark</code></li>\n</ul>\n"}},{"text":"%md\n### Spark Context\nYour connection to the spark cluster.  Allows to\n* Run jobs\n* Accessing service s.a.  Task scheduler\n* Creating RDDs\n* ...","dateUpdated":"Jan 10, 2016 11:58:27 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452434761525_-652895653","id":"20160110-090601_718789663","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Spark Context</h3>\n<p>Your connection to the spark cluster.  Allows to</p>\n<ul>\n<li>Run jobs</li>\n<li>Accessing service s.a.  Task scheduler</li>\n<li>Creating RDDs</li>\n<li>&hellip;</li>\n</ul>\n"},"dateCreated":"Jan 10, 2016 9:06:01 AM","dateStarted":"Jan 10, 2016 11:58:27 AM","dateFinished":"Jan 10, 2016 11:58:27 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:21"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452460299300_2045553358","id":"20160110-161139_1168589545","dateCreated":"Jan 10, 2016 4:11:39 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:735","text":"%md\n## Notebooks\nWell known from Jupyter (formerly known as ipython notebook)\n* **Zeppelin** used here\n    * built on JVM\n    * Spark first\n    * Apache incubator\n* **Jupyter** (not tried)\n    * Very mature\n    * huge community\n* **Databricks**\n    * Great\n    * Commercial $$$\n* **More**\n    * [Beaker notebook](http://beakernotebook.com/)\n    * [spark-notebook](https://github.com/andypetrella/spark-notebook/)\n    * ...","dateUpdated":"Jan 10, 2016 4:41:52 PM","dateFinished":"Jan 10, 2016 4:34:38 PM","dateStarted":"Jan 10, 2016 4:34:38 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Notebooks</h2>\n<p>Well known from Jupyter (formerly known as ipython notebook)</p>\n<ul>\n<li><strong>Zeppelin</strong> used here<ul>\n<li>built on JVM</li>\n<li>Spark first</li>\n<li>Incubating</li>\n</ul>\n</li>\n<li><strong>Jupyter</strong> (not tried)<ul>\n<li>Very mature</li>\n<li>huge community</li>\n</ul>\n</li>\n<li><strong>Databricks</strong><ul>\n<li>Great</li>\n<li>Commercial $$$</li>\n</ul>\n</li>\n<li><strong>More</strong><ul>\n<li><a href=\"http://beakernotebook.com/\">Beaker notebook</a></li>\n<li><a href=\"https://github.com/andypetrella/spark-notebook/\">spark-notebook</a></li>\n<li>&hellip;</li>\n</ul>\n</li>\n</ul>\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452461686710_1015570162","id":"20160110-163446_204675287","dateCreated":"Jan 10, 2016 4:34:46 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:759","text":"%md\n### Zeppelin\neasy install\n* download `git clone https://github.com/apache/incubator-zeppelin/`\n* `cd incubator-zeppelin/`\n* `mvn clean package -Pspark-1.5 -DskipTests`\n* `./bin/zeppelin-daemon.sh start`\n* go to `http://localhost:8080/`","dateUpdated":"Jan 10, 2016 4:47:11 PM","dateFinished":"Jan 10, 2016 4:47:11 PM","dateStarted":"Jan 10, 2016 4:47:11 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Zeppelin</h3>\n<p>easy install</p>\n<ul>\n<li>download <code>git clone https://github.com/apache/incubator-zeppelin/</code></li>\n<li><code>cd incubator-zeppelin/</code></li>\n<li><code>mvn clean package -Pspark-1.5 -DskipTests</code></li>\n<li><code>./bin/zeppelin-daemon.sh start</code></li>\n<li>go to <code>http://localhost:8080/</code></li>\n</ul>\n"}},{"text":"%md\n##RDD\nResillient Distributed Datasets are the basic abstractions in Spark.\nThey are immutable and distributed (partionned) over the available cluster.\nCalculation can be performed in parallel over the partitionned elements.","dateUpdated":"Jan 10, 2016 8:43:20 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1451922045258_1880894960","id":"20160104-104045_1156027160","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>RDD</h2>\n<p>Resillient Distributed Datasets are the basic abstractions in Spark.\n<br  />They are immutable and distributed (partionned) over the available cluster.\n<br  />Calculation can be performed in parallel over the partitionned elements.</p>\n"},"dateCreated":"Jan 4, 2016 10:40:45 AM","dateStarted":"Jan 10, 2016 8:43:20 AM","dateFinished":"Jan 10, 2016 8:43:20 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:22"},{"text":"%md\nRDD can be created from a sequence of elements.","dateUpdated":"Jan 10, 2016 4:08:46 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452433421695_-1115491357","id":"20160110-084341_1759149366","result":{"code":"SUCCESS","type":"HTML","msg":"<p>RDD can be created from sequence of elements.</p>\n"},"dateCreated":"Jan 10, 2016 8:43:41 AM","dateStarted":"Jan 10, 2016 8:44:42 AM","dateFinished":"Jan 10, 2016 8:44:42 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:23"},{"text":"val wordsRDD = sc.parallelize(List(\"fish\", \"cats\", \"poutine\"))","dateUpdated":"Jan 4, 2016 8:09:43 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1451921891182_-77875371","id":"20160104-103811_1298335970","result":{"code":"SUCCESS","type":"TEXT","msg":"wordsRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[83] at parallelize at <console>:23\n"},"dateCreated":"Jan 4, 2016 10:38:11 AM","dateStarted":"Jan 4, 2016 10:39:33 AM","dateFinished":"Jan 4, 2016 10:39:33 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:24"},{"text":"//look at the results\nwordsRDD.take(2)","dateUpdated":"Jan 10, 2016 8:49:51 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1451921895821_953267006","id":"20160104-103815_281323834","result":{"code":"SUCCESS","type":"TEXT","msg":"res3: Array[String] = Array(fish, cats)\n"},"dateCreated":"Jan 4, 2016 10:38:15 AM","dateStarted":"Jan 4, 2016 10:39:47 AM","dateFinished":"Jan 4, 2016 10:39:48 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25"},{"text":"%md\n#### Reading from a text file\nEach line is an element","dateUpdated":"Jan 10, 2016 8:50:57 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452433806885_-1279275085","id":"20160110-085006_822791139","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Reading from a text file</h4>\n<p>Each line is an element</p>\n"},"dateCreated":"Jan 10, 2016 8:50:06 AM","dateStarted":"Jan 10, 2016 8:50:57 AM","dateFinished":"Jan 10, 2016 8:50:57 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:26"},{"text":"//Lazy: if path is wrong, it will show only at an action (like take)\n//val linesRDD = sc.textFile(\"/Users/jrajott1/work/local_spark/data/airbnb/mtl_listingsWRONGNAME.csv\")\nval linesRDD = sc.textFile(\"/Users/jrajott1/work/local_spark/data/airbnb/mtl_listings.csv\")","dateUpdated":"Jan 10, 2016 8:52:11 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1451921987810_-918781580","id":"20160104-103947_23798420","result":{"code":"SUCCESS","type":"TEXT","msg":"linesRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1112] at textFile at <console>:25\n"},"dateCreated":"Jan 4, 2016 10:39:47 AM","dateStarted":"Jan 10, 2016 8:52:11 AM","dateFinished":"Jan 10, 2016 8:52:11 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:27"},{"text":"linesRDD.count","dateUpdated":"Jan 10, 2016 8:52:16 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1451938838059_-632042876","id":"20160104-152038_196242379","result":{"code":"SUCCESS","type":"TEXT","msg":"res144: Long = 8986\n"},"dateCreated":"Jan 4, 2016 3:20:38 PM","dateStarted":"Jan 10, 2016 8:52:16 AM","dateFinished":"Jan 10, 2016 8:52:16 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28"},{"text":"linesRDD.take(5).foreach {println}","dateUpdated":"Jan 10, 2016 8:52:31 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1451922149520_1942424135","id":"20160104-104229_602788605","result":{"code":"SUCCESS","type":"TEXT","msg":"id,name,host_id,host_name,neighbourhood_group,neighbourhood,latitude,longitude,room_type,price,minimum_nights,number_of_reviews,last_review,reviews_per_month,calculated_host_listings_count,availability_365\n7771742,Cozy and Bright Room in Mile-end,40880361,Aletha,,Outremont,45.52230324180573,-73.60671958096545,Private room,37,1,1,2015-08-27,0.81,1,365\n959727,Superbe appartement à Montréal,1792119,Christine,,Outremont,45.5197179590061,-73.61050026481905,Entire home/apt,78,21,0,,,1,122\n6588054,Cosy & elegant apt (Outremont),10810386,Franck,,Outremont,45.5069662162218,-73.61655040181864,Entire home/apt,110,3,7,2015-08-30,1.79,1,122\n4532643,Spacious 2 bedroom GREAT location!,20764599,Amelie&Manu,,Outremont,45.51587393670735,-73.59125450895273,Entire home/apt,90,1,3,2015-09-08,0.33,2,272\n"},"dateCreated":"Jan 4, 2016 10:42:29 AM","dateStarted":"Jan 10, 2016 8:52:31 AM","dateFinished":"Jan 10, 2016 8:52:31 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29"},{"text":"%md\n## Reading data\nSpark SQL provides inbuilt support for 3 types of data sources:\n* Parquet (default)\n* Json\n* Jdbc\n\nexample:\n`val people = sqlContext.read.json(\"people.json\")`\n\nWarning: from spark docs (about json):\nEach line must contain a separate, self-contained valid JSON object. As a consequence, a regular multi-line JSON file will most often fail.\n\n### The cheating's way\nuse non-spark tool, like pandas to read the data and turn it into a spark-native format.\nCaveat: limited by memory","dateUpdated":"Jan 10, 2016 5:10:03 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452433984120_-239021328","id":"20160110-085304_1686593696","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Reading data</h2>\n<p>Spark SQL provides inbuilt support for 3 types of data sources:</p>\n<ul>\n<li>Parquet (default)</li>\n<li>Json</li>\n<li>Jdbc</li>\n</ul>\n<p>example:\n<br  /><code>val people = sqlContext.read.json(\"people.json\")</code></p>\n<p>Warning: from spark docs (about json):\n<br  />Each line must contain a separate, self-contained valid JSON object. As a consequence, a regular multi-line JSON file will most often fail.</p>\n<h3>The cheating's way</h3>\n<p>use non-spark tool, like pandas to read the data and turn it into a spark-native format.\n<br  />Caveat: limited by memory</p>\n"},"dateCreated":"Jan 10, 2016 8:53:04 AM","dateStarted":"Jan 10, 2016 5:10:03 PM","dateFinished":"Jan 10, 2016 5:10:03 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30","focus":true},{"text":"%md\n##csv\n###Brute force: using case class\nBy looking at the rows, define a case classe where you infer the type.  In case of doubt, use `String`\nWe can take any subsample of the fields.","dateUpdated":"Jan 10, 2016 5:05:24 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452430406522_970507218","id":"20160110-075326_993963702","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>csv</h2>\n<h3>Brute force: using case class</h3>\n<p>By looking at the rows, define a case classe where you infer the type.  In case of doubt, use <code>String</code>\n<br  />We can take any subsample of the fields.</p>\n"},"dateCreated":"Jan 10, 2016 7:53:26 AM","dateStarted":"Jan 10, 2016 5:05:24 PM","dateFinished":"Jan 10, 2016 5:05:24 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:31","focus":true},{"text":"case class Listing(id: Integer,name: String, neighbourhood: String, latitude: Double, longitude: Double)","dateUpdated":"Jan 10, 2016 8:02:55 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452430506929_1556818569","id":"20160110-075506_1146640351","result":{"code":"SUCCESS","type":"TEXT","msg":"defined class Listing\n"},"dateCreated":"Jan 10, 2016 7:55:06 AM","dateStarted":"Jan 10, 2016 8:02:55 AM","dateFinished":"Jan 10, 2016 8:02:55 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:32"},{"text":"val listings = linesRDD.map(s => s.split(\",\")).filter(s => s(0) != \"id\").map(\n    s => Listing(s(0).toInt,\n            s(1),\n            s(5),\n            s(6).toFloat,\n            s(7).toFloat\n        )\n)","dateUpdated":"Jan 10, 2016 8:10:32 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452431021119_-1676108","id":"20160110-080341_32911467","result":{"code":"SUCCESS","type":"TEXT","msg":"listings: org.apache.spark.rdd.RDD[Listing] = MapPartitionsRDD[1106] at map at <console>:27\n"},"dateCreated":"Jan 10, 2016 8:03:41 AM","dateStarted":"Jan 10, 2016 8:10:32 AM","dateFinished":"Jan 10, 2016 8:10:32 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33"},{"text":"listings.take(3).foreach {println}","dateUpdated":"Jan 10, 2016 8:10:35 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452431243720_-1075419988","id":"20160110-080723_764952913","result":{"code":"SUCCESS","type":"TEXT","msg":"Listing(7771742,Cozy and Bright Room in Mile-end,Outremont,45.52230453491211,-73.60671997070312)\nListing(959727,Superbe appartement à Montréal,Outremont,45.519718170166016,-73.6104965209961)\nListing(6588054,Cosy & elegant apt (Outremont),Outremont,45.50696563720703,-73.61654663085938)\n"},"dateCreated":"Jan 10, 2016 8:07:23 AM","dateStarted":"Jan 10, 2016 8:10:35 AM","dateFinished":"Jan 10, 2016 8:10:35 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:34"},{"text":"%md\n### Automize access csv-to-rdd fields\nIf you want to ease the access the the elements ","dateUpdated":"Jan 10, 2016 5:07:45 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452431486254_1145318371","id":"20160110-081126_1561614828","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Automize access csv-to-rdd fields</h3>\n<p>If you want to ease the access the the elements</p>\n"},"dateCreated":"Jan 10, 2016 8:11:26 AM","dateStarted":"Jan 10, 2016 5:07:45 PM","dateFinished":"Jan 10, 2016 5:07:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:35","focus":true},{"text":"//reading the csv\nval headersAndRows = linesRDD.map(line => line.split(\",\").map(_.trim))\nval header = headersAndRows.first\nval data = headersAndRows.filter(_(0) != header(0))\nval maps = data.map(splits => header.zip(splits).toMap)","dateUpdated":"Jan 10, 2016 8:21:55 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1451944529074_-310039292","id":"20160104-165529_581925540","result":{"code":"SUCCESS","type":"TEXT","msg":"headersAndRows: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[1097] at map at <console>:26\nheader: Array[String] = Array(id, name, host_id, host_name, neighbourhood_group, neighbourhood, latitude, longitude, room_type, price, minimum_nights, number_of_reviews, last_review, reviews_per_month, calculated_host_listings_count, availability_365)\ndata: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[1098] at filter at <console>:29\nmaps: org.apache.spark.rdd.RDD[scala.collection.immutable.Map[String,String]] = MapPartitionsRDD[1099] at map at <console>:31\n"},"dateCreated":"Jan 4, 2016 4:55:29 PM","dateStarted":"Jan 10, 2016 7:42:54 AM","dateFinished":"Jan 10, 2016 7:42:54 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36"},{"text":"val results = maps.filter(map => map(\"id\") == \"6588054\")\nresults.take(5).foreach(println)","dateUpdated":"Jan 10, 2016 7:44:53 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1451950707870_676555108","id":"20160104-183827_981446984","result":{"code":"SUCCESS","type":"TEXT","msg":"results: org.apache.spark.rdd.RDD[scala.collection.immutable.Map[String,String]] = MapPartitionsRDD[1100] at filter at <console>:33\nMap(neighbourhood_group -> , name -> Cosy & elegant apt (Outremont), room_type -> Entire home/apt, latitude -> 45.5069662162218, price -> 110, host_name -> Franck, longitude -> -73.61655040181864, calculated_host_listings_count -> 1, availability_365 -> 122, id -> 6588054, number_of_reviews -> 7, host_id -> 10810386, last_review -> 2015-08-30, reviews_per_month -> 1.79, minimum_nights -> 3, neighbourhood -> Outremont)\n"},"dateCreated":"Jan 4, 2016 6:38:27 PM","dateStarted":"Jan 10, 2016 7:44:53 AM","dateFinished":"Jan 10, 2016 7:44:53 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37"},{"text":"%md\n### Spark packages\nCommunity contribution to spark [here](http://spark-packages.org/)\n\n#### example [spark-csv](https://github.com/databricks/spark-csv)\nFrom the shell\n`$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.10:1.3.0`\n**Note**: If inside Zeppelin notebook (see first cell of this notebook)","dateUpdated":"Jan 10, 2016 5:07:03 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452441759389_-618797341","id":"20160110-110239_769953840","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Spark packages</h3>\n<p>Community contribution to spark <a href=\"http://spark-packages.org/\">here</a></p>\n<h4>example <a href=\"https://github.com/databricks/spark-csv\">spark-csv</a></h4>\n<p>From the shell\n<br  /><code>$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.10:1.3.0</code>\n<br  /><strong>Note</strong>: Inside Zeppelin notebook (see first cell of this notebook)</p>\n"},"dateCreated":"Jan 10, 2016 11:02:39 AM","dateStarted":"Jan 10, 2016 5:06:47 PM","dateFinished":"Jan 10, 2016 5:06:47 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38","focus":true},{"text":"val df = sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types //.option(\"quote\", \"\\\"\")\n    .option(\"mode\", \"DROPMALFORMED\")\n    .load(\"/Users/jrajott1/work/local_spark/data/airbnb/mtl_listings.csv\")","dateUpdated":"Jan 10, 2016 11:15:34 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452442475262_-1780931655","id":"20160110-111435_1087783536","result":{"code":"SUCCESS","type":"TEXT","msg":"df: org.apache.spark.sql.DataFrame = [id: string, name: string, host_id: string, host_name: string, neighbourhood_group: string, neighbourhood: string, latitude: double, longitude: string, room_type: string, price: int, minimum_nights: int, number_of_reviews: int, last_review: string, reviews_per_month: double, calculated_host_listings_count: int, availability_365: int]\n"},"dateCreated":"Jan 10, 2016 11:14:35 AM","dateStarted":"Jan 10, 2016 11:15:34 AM","dateFinished":"Jan 10, 2016 11:15:35 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:39"},{"text":"df","dateUpdated":"Jan 10, 2016 11:33:50 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452443627649_-606619481","id":"20160110-113347_264721300","result":{"code":"SUCCESS","type":"TEXT","msg":"res161: org.apache.spark.sql.DataFrame = [id: string, name: string, host_id: string, host_name: string, neighbourhood_group: string, neighbourhood: string, latitude: double, longitude: string, room_type: string, price: int, minimum_nights: int, number_of_reviews: int, last_review: string, reviews_per_month: double, calculated_host_listings_count: int, availability_365: int]\n"},"dateCreated":"Jan 10, 2016 11:33:47 AM","dateStarted":"Jan 10, 2016 11:33:51 AM","dateFinished":"Jan 10, 2016 11:33:51 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:40"},{"text":"%md\n##DataFrames\n* Introduced in spark 1.3\n* Build on RDD\n* \"Prefered abstraction in Spark\"\n    * Simpler\n    * Better optimized (especially for python,r... any non-scala/jave)\n* Inspired by R and Pandas dataframe **but immutable**\n\n###Creation\n* by parallelizing existing collections (e.g. RDDs, Pandas DataFrames)\n* by transforming and existing DataFrame\n* from files in storage system (e.g. parquet files)","dateUpdated":"Jan 10, 2016 10:54:39 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1451955280861_1711673547","id":"20160104-195440_1687924753","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>DataFrames</h2>\n<ul>\n<li>Introduced in spark 1.3</li>\n<li>Build on RDD</li>\n<li>&ldquo;Prefered abstraction in Spark&rdquo;<ul>\n<li>Simpler</li>\n<li>Better optimized (especially for python,r&hellip; any non-scala/jave)</li>\n</ul>\n</li>\n<li>Inspired by R and Pandas dataframe <strong>but immutable</strong></li>\n</ul>\n<h3>Creation</h3>\n<ul>\n<li>by parallelizing existing collections (e.g. RDDs, Pandas DataFrames)</li>\n<li>by transforming and existing DataFrame</li>\n<li>from files in storage system (e.g. parquet files)</li>\n</ul>\n"},"dateCreated":"Jan 4, 2016 7:54:40 PM","dateStarted":"Jan 10, 2016 10:54:39 AM","dateFinished":"Jan 10, 2016 10:54:39 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41"},{"text":"%md\n### Schema","dateUpdated":"Jan 4, 2016 8:39:05 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1451955312490_742152292","id":"20160104-195512_588577350","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Schema</h3>\n"},"dateCreated":"Jan 4, 2016 7:55:12 PM","dateStarted":"Jan 4, 2016 8:39:05 PM","dateFinished":"Jan 4, 2016 8:39:05 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:42"},{"text":"df.printSchema","dateUpdated":"Jan 10, 2016 11:34:42 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452443677974_508275563","id":"20160110-113437_2098714142","result":{"code":"SUCCESS","type":"TEXT","msg":"root\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- host_id: string (nullable = true)\n |-- host_name: string (nullable = true)\n |-- neighbourhood_group: string (nullable = true)\n |-- neighbourhood: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: string (nullable = true)\n |-- room_type: string (nullable = true)\n |-- price: integer (nullable = true)\n |-- minimum_nights: integer (nullable = true)\n |-- number_of_reviews: integer (nullable = true)\n |-- last_review: string (nullable = true)\n |-- reviews_per_month: double (nullable = true)\n |-- calculated_host_listings_count: integer (nullable = true)\n |-- availability_365: integer (nullable = true)\n\n"},"dateCreated":"Jan 10, 2016 11:34:37 AM","dateStarted":"Jan 10, 2016 11:34:42 AM","dateFinished":"Jan 10, 2016 11:34:42 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:43"},{"text":"//Include one or two example of dataframe manipulation\n//Then advertize to Luis' presentation","dateUpdated":"Jan 10, 2016 11:36:40 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452443698032_1556913885","id":"20160110-113458_665838262","dateCreated":"Jan 10, 2016 11:34:58 AM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:44"},{"text":"%md\n## Spark SQL\n* SQLContext (default) support a “rich subset dialect of SQL 92”\n* HiveContext \"dialect is hiveql\" richer than SQLContext\n","dateUpdated":"Jan 10, 2016 11:55:50 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452443777515_451438350","id":"20160110-113617_1975690843","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Spark SQL</h2>\n<ul>\n<li>SQLContext (default) support a “rich subset dialect of SQL 92”</li>\n<li>HiveContext &ldquo;dialect is hiveql&rdquo; richer than SQLContext</li>\n</ul>\n"},"dateCreated":"Jan 10, 2016 11:36:17 AM","dateStarted":"Jan 10, 2016 11:55:50 AM","dateFinished":"Jan 10, 2016 11:55:50 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:45"},{"text":"%md\nLet's create a table by registering the dataframe\n* Same performance as DataFrames\n* Better performance as RDDs","dateUpdated":"Jan 10, 2016 11:55:58 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452444925403_-489989658","id":"20160110-115525_1174858896","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Let's create a table by registering the dataframe</p>\n<ul>\n<li>Same performance as DataFrames</li>\n<li>Better performance as RDDs</li>\n</ul>\n"},"dateCreated":"Jan 10, 2016 11:55:25 AM","dateStarted":"Jan 10, 2016 11:55:58 AM","dateFinished":"Jan 10, 2016 11:55:58 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:46"},{"text":"df.registerTempTable(\"airbnb\")","dateUpdated":"Jan 10, 2016 11:43:22 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452444186174_-1944578585","id":"20160110-114306_682989923","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Jan 10, 2016 11:43:06 AM","dateStarted":"Jan 10, 2016 11:43:22 AM","dateFinished":"Jan 10, 2016 11:43:22 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:47"},{"text":"sqlContext.sql(\n    \"\"\"\n        SELECT name, price\n        FROM airbnb\n        LIMIT 5\n    \"\"\"\n).show","dateUpdated":"Jan 10, 2016 11:45:48 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452444208819_750248683","id":"20160110-114328_578273051","result":{"code":"SUCCESS","type":"TEXT","msg":"+--------------------+-----+\n|                name|price|\n+--------------------+-----+\n|Cozy and Bright R...|   37|\n|Superbe apparteme...|   78|\n|Cosy & elegant ap...|  110|\n|Spacious 2 bedroo...|   90|\n|Beautiful typical...|  116|\n+--------------------+-----+\n\n"},"dateCreated":"Jan 10, 2016 11:43:28 AM","dateStarted":"Jan 10, 2016 11:45:48 AM","dateFinished":"Jan 10, 2016 11:45:48 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:48"},{"text":"%md\nOr in zeppelin just make the cell sql","dateUpdated":"Jan 10, 2016 11:46:36 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452444363769_-1726766380","id":"20160110-114603_694588019","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Or in zeppelin just make the cell sql</p>\n"},"dateCreated":"Jan 10, 2016 11:46:03 AM","dateStarted":"Jan 10, 2016 11:46:36 AM","dateFinished":"Jan 10, 2016 11:46:36 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:49"},{"text":"%sql\nSELECT AVG(reviews_per_month) monthly_reviews, neighbourhood\nFROM airbnb\nGROUP BY neighbourhood\nORDER BY monthly_reviews DESC\nLIMIT 10","dateUpdated":"Jan 10, 2016 11:52:07 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"neighbourhood","index":1,"aggr":"sum"}],"values":[{"name":"monthly_reviews","index":0,"aggr":"sum"}],"groups":[],"scatter":{"yAxis":{"name":"neighbourhood","index":1,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452444398560_-664043945","id":"20160110-114638_662684415","result":{"code":"SUCCESS","type":"TABLE","msg":"monthly_reviews\tneighbourhood\n2.1234569536423855\tVille-Marie\n1.8704347826086958\tLaSalle\n1.87\tMontréal-Est\n1.7061538461538461\tAnjou\n1.6683689421573733\tLe Plateau-Mont-Royal\n1.5553571428571427\tLachine\n1.4924234693877545\tLe Sud-Ouest\n1.4764285714285716\tDorval\n1.4555555555555553\tMont-Royal\n1.4376689189189185\tMercier-Hochelaga-Maisonneuve\n","comment":"","msgTable":[[{"key":"neighbourhood","value":"2.1234569536423855"},{"key":"neighbourhood","value":"Ville-Marie"}],[{"value":"1.8704347826086958"},{"value":"LaSalle"}],[{"value":"1.87"},{"value":"Montréal-Est"}],[{"value":"1.7061538461538461"},{"value":"Anjou"}],[{"value":"1.6683689421573733"},{"value":"Le Plateau-Mont-Royal"}],[{"value":"1.5553571428571427"},{"value":"Lachine"}],[{"value":"1.4924234693877545"},{"value":"Le Sud-Ouest"}],[{"value":"1.4764285714285716"},{"value":"Dorval"}],[{"value":"1.4555555555555553"},{"value":"Mont-Royal"}],[{"value":"1.4376689189189185"},{"value":"Mercier-Hochelaga-Maisonneuve"}]],"columnNames":[{"name":"monthly_reviews","index":0,"aggr":"sum"},{"name":"neighbourhood","index":1,"aggr":"sum"}],"rows":[["2.1234569536423855","Ville-Marie"],["1.8704347826086958","LaSalle"],["1.87","Montréal-Est"],["1.7061538461538461","Anjou"],["1.6683689421573733","Le Plateau-Mont-Royal"],["1.5553571428571427","Lachine"],["1.4924234693877545","Le Sud-Ouest"],["1.4764285714285716","Dorval"],["1.4555555555555553","Mont-Royal"],["1.4376689189189185","Mercier-Hochelaga-Maisonneuve"]]},"dateCreated":"Jan 10, 2016 11:46:38 AM","dateStarted":"Jan 10, 2016 11:51:55 AM","dateFinished":"Jan 10, 2016 11:51:55 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:50"},{"text":"//we have hivecontext\nsqlContext","dateUpdated":"Jan 10, 2016 11:36:55 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1452443812736_-649266956","id":"20160110-113652_1211575892","result":{"code":"SUCCESS","type":"TEXT","msg":"res169: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@38c34561\n"},"dateCreated":"Jan 10, 2016 11:36:52 AM","dateStarted":"Jan 10, 2016 11:36:55 AM","dateFinished":"Jan 10, 2016 11:36:55 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:51"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1451922459861_-464507500","id":"20160104-104739_791390026","dateCreated":"Jan 4, 2016 10:47:39 AM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:52","dateUpdated":"Jan 10, 2016 3:50:05 PM","text":"%md\n# TODO\n* writing\n* ressources"}],"name":"gettingStarted","id":"2BAR5MZVD","angularObjects":{"2B82J2QHM":[],"2B71QZCFU":[],"2BACEBUSP":[],"2BACENG9M":[],"2B7KY97EH":[],"2B94T8F9P":[],"2B9SCRUHV":[],"2B9SFPK8B":[],"2BAQ8YFWU":[],"2B728KH8H":[],"2B8P1DPQE":[],"2B9THPJR4":[],"2B9C8E92A":[],"2B9W8UZXG":[]},"config":{"looknfeel":"default"},"info":{}}